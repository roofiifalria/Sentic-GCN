{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3pE8lVQLOm0p"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roofiifalria/Sentic-GCN/blob/main/GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation dan Data Prepocessing\n"
      ],
      "metadata": {
        "id": "3pE8lVQLOm0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk spacy scikit-learn tensorflow-cpu transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import warnings\n",
        "from tabulate import tabulate\n",
        "import spacy\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpOxhE5KHWpR",
        "outputId": "01382dcf-e595-4cb0-9aa4-7ebd575d9211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tensorflow-cpu in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow-cpu) (13.9.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow-cpu) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow-cpu) (0.13.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow-cpu) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow-cpu) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-cpu) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords and punkt for tokenization\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load your data\n",
        "train_data = pd.read_csv('train_GCN.csv')\n",
        "train_data = train_data.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "test_data = pd.read_csv('test_GCN.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vviGJoX4HZpm",
        "outputId": "2e943dd8-9917-48fa-a01e-4fff860432d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgjHeIf4-Et"
      },
      "outputs": [],
      "source": [
        "# Function for text preprocessing\n",
        "def preprocess_text(sentence):\n",
        "    sentence = sentence.lower()  # 1. Lowercasing\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence)  # 2. Remove punctuation\n",
        "\n",
        "    # Check if the number of words exceeds 512\n",
        "    words = word_tokenize(sentence)\n",
        "    if len(words) > 512:\n",
        "        return None  # Or return '' if you prefer\n",
        "\n",
        "    words = [word for word in words if word not in stop_words]  # 4. Remove stopwords\n",
        "    doc = nlp(\" \".join(words))  # 5. Lemmatization\n",
        "    lemmatized_words = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing data\n",
        "train_data.dropna(subset=['review'], inplace=True)\n",
        "test_data.dropna(subset=['review'], inplace=True)\n",
        "\n",
        "train_data['cleaned_sentence'] = train_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows where cleaned_sentence is None\n",
        "train_data = train_data[train_data['cleaned_sentence'].notna()]\n",
        "\n",
        "# Convert sentiments to numbers\n",
        "train_data['sentiment'] = train_data['sentiment'].replace({'positive': 0, 'neutral': 1, 'negative': 2})\n",
        "\n",
        "# Visualize sentiment distribution\n",
        "sns.countplot(data=train_data, x='sentiment')\n",
        "plt.title('Sentiment Distribution in Training Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUCX_zMnHmjl",
        "outputId": "cbdf7a12-8674-4e2c-9837-4a668994014d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-28f55aed4ee9>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['sentiment'] = train_data['sentiment'].replace({'positive': 0, 'neutral': 1, 'negative': 2})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print output\n",
        "print(\"\\nTraining Data After Preprocessing:\")\n",
        "print(tabulate(train_data.head(), headers='keys', tablefmt='sqlite'))  # Display first few rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OFxEDN-WClP",
        "outputId": "f17abe96-8d6d-4a59-ce92-08b3b47c7dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data After Preprocessing:\n",
            "    review                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             sentiment  cleaned_sentence\n",
            "--  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " 0  Even a awful 1 is to much for this film, everything form start to finish made you cringe. I don't think it would be possible to cram more overly clichéd moments, into one piece of mind numbingly numbingly waste of film.<br /><br />Prisoner cell block H meets Thunderbirds, hell even Virgil's expressions were more life like than his son.<br /><br />I haven't even finished watching this and I'm on here now.... Oh no, the cheesy clapping of 3 actors and a backdrop done by a child with adobe premiere. This truly is the end of my \"I've started so I'll finish watching it\" phase.<br /><br />Oh joy, the credits have come to rescue me. (and relax)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2  even awful 1 much film everything form start finish make cringe do not think would possible cram overly clichéd moment one piece mind numbingly numbingly waste filmbr br prisoner cell block h meet thunderbird hell even virgil expression life like sonbr br have not even finish watch I m oh cheesy clap 3 actor backdrop do child adobe premiere truly end I ve start ill finish watch phasebr br oh joy credit come rescue relax\n",
            " 2  I've seen a few movies in my time, but this one is exceptional. You'll have to watch it more than once to truly appreciate it, it is emotionally very complex, it explores love and passion at it's most extreme and it's cinematography is just breathtaking. The character of the Count is intensely passionate and tragic without him having to raise his voice or indeed leave his bed, the film is perfectly cast and perfectly acted. The film has a sort of mathematical precision and perfection to it which is rare these days. It combines action, love, tragedy, drama and politics all in one. This movie is unmissable, all the hype surrounding it and all the awards cannot begin to do it any justice. Hats off to Michael Ondaatje for writing the incredible book on which it is based.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0  I ve see movie time one exceptional you ll watch truly appreciate emotionally complex explore love passion extreme cinematography breathtaking character count intensely passionate tragic without raise voice indeed leave bed film perfectly cast perfectly act film sort mathematical precision perfection rare day combine action love tragedy drama politic one movie unmissable hype surround award begin justice hat michael ondaatje write incredible book base\n",
            " 3  This movie truly captures the feeling of freedom.......and what the freedom of your own integrity is worth....in the most delightful, light-hearted way. Not a serious, but hilarious adventure.<br /><br />The story mirrors life. We don't always get what we want right away but we find out we get what we need to to understand why we didn't get what we wanted....which results in us getting more than we thought we would get! You will get this once you see the movie. <br /><br />And this movie is truly about finding love and knowing one has found it and that it totally changed one's life.<br /><br />It is one of my all time favorites......not easy to find but worth the hunt.........I guarantee you will watch it more than once!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0  movie truly capture feel freedomand freedom integrity worthin delightful lighthearted way serious hilarious adventurebr br story mirror life do not always get want right away find get need understand do not get wantedwhich result we getting think would get get see movie br br movie truly find love know one find totally change one lifebr br one time favoritesnot easy find worth hunti guarantee watch\n",
            " 4  There's not much anyone can say about this flick....the plot is quite simple: Two police officers (who also happen to be lovers) are using a brothel as a stakeout in order to catch a criminal, with the help of the \"lady of the house\", played by hardcore pornstar Chloe. As anyone can guess, there's a few plot twists and some blurred alliances, but the writing was just horrible, even for a softcore movie.<br /><br />I've read some previous posts about Nicole Hilbig's accent (she plays the female cop). Yes, it's hard to understand what she's saying at times, but I think I've placed it. I did some sniffing around....I think she's from Germany, hence her odd-sounding accent. She makes an impression even without speaking, however...she's got a great looking body.<br /><br />There were a couple of \"from behind\" sex scenes in this movie that were quite graphic for a softcore film....excellent work there. The three-way scene toward the end wasn't bad either.<br /><br />*SPOILER ALERT*<br /><br />I kinda knew the female cop was gonna turn into a part-time call girl at the end. She enjoyed her three-way WAY too much.<br /><br />*END SPOILER*<br /><br />I'm not gonna nitpick about the story TOO much, seeing as this is a low-budget, direct-to-video softcore flick. However, it just seems like I've seen way too many movies in this genre with a similar type of storyline.<br /><br />Women: B (Chloe and Hilbig were okay as the eye-candy) Sex: B+ (scenes were kinda short, but good) Story: C (a recycled plot, but whatever works, eh?) Overall: B-            0  there s much anyone say flickthe plot quite simple two police officer also happen lover use brothel stakeout order catch criminal help lady house play hardcore pornstar chloe anyone guess there s plot twist blur alliance write horrible even softcore moviebr br I ve read previous post nicole hilbigs accent play female cop yes hard understand she s say time think I ve place sniff aroundi think she s germany hence oddsounde accent make impression even without speak howevershe get great look bodybr br couple behind sex scene movie quite graphic softcore filmexcellent work threeway scene toward end be not bad eitherbr br spoiler alertbr br kinda know female cop go to turn parttime call girl end enjoy threeway way muchbr br end spoilerbr br i m go to nitpick story much see lowbudget directtovideo softcore flick however seem like I ve see way many movie genre similar type storylinebr br women b chloe hilbig okay eyecandy sex b scene kinda short good story c recycle plot whatever work eh overall b\n",
            " 6  \"Checking Out\" is a very witty and honest portrayal of a bizarre family that happens to be Jewish. Judaism plays virtually no role in the film, but American Jewish culture & behavior gets thoroughly sent up... it a loving way. I wish the movie dealt with the religious perspective on the topics it explores, because I think that would have been interesting. <br /><br />I've never been a Columbo fan so I wasn't familiar with Peter Falk - he's a lot of fun to watch. It's great to see Judge Reinhold, Laura San Giacomo & David Paymer again - why don't they work more? They're all hilarious. The script is terrific with a lot of memorable one-liners I'll be sure to use with my own family. Watch for Gavin McLeod (Captain Stubing!) as the doorman.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0  check witty honest portrayal bizarre family happen jewish judaism play virtually role film american jewish culture behavior get thoroughly send love way wish movie deal religious perspective topic explore think would interesting br br I ve never columbo fan be not familiar peter falk he s lot fun watch great see judge reinhold laura san giacomo david paymer do not work they re hilarious script terrific lot memorable oneliner ill sure use family watch gavin mcleod captain stub doorman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT and BiLSTM"
      ],
      "metadata": {
        "id": "zfSWjlh9R4b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # *2 karena bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.linear(lstm_out)\n",
        "        return out\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "def get_sentence_vectors(sentence, bert_model, tokenizer):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**inputs)\n",
        "\n",
        "    # Ambil semua token embeddings dan juga ID token untuk mendeteksi kata-kata\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
        "\n",
        "    # Hapus [CLS] dan [SEP] dari tokens dan embeddings\n",
        "    if \"[CLS]\" in tokens: tokens.remove(\"[CLS]\")\n",
        "    if \"[SEP]\" in tokens: tokens.remove(\"[SEP]\")\n",
        "\n",
        "    embeddings = bert_outputs.last_hidden_state.squeeze(0)[1:-1]  # Hapus embedding untuk [CLS] dan [SEP]\n",
        "\n",
        "    return tokens, embeddings\n",
        "\n",
        "def create_contextualized_word_representations(sentence, bert_model, tokenizer, bilstm_model):\n",
        "    # Dapatkan token dan vektor kalimat dari BERT\n",
        "    tokens, sentence_vector = get_sentence_vectors(sentence, bert_model, tokenizer)\n",
        "\n",
        "    # Dapatkan representasi kontekstual dari BiLSTM\n",
        "    contextualized_word_representations = bilstm_model(sentence_vector.unsqueeze(0))  # Menambah dimensi batch\n",
        "\n",
        "    return tokens, contextualized_word_representations.squeeze(0)  # Menghapus dimensi batch\n",
        "\n",
        "\n",
        "# Function to build sentence graph\n",
        "def build_sentence_graph(sentence, contextualized_representations, nlp):\n",
        "    doc = nlp(sentence)\n",
        "    num_words = len(doc)\n",
        "\n",
        "    # Initialize adjacency matrix\n",
        "    A = torch.zeros((num_words, num_words))\n",
        "\n",
        "    # Fill adjacency matrix based on dependency parse\n",
        "    for token in doc:\n",
        "        A[token.i, token.i] = 1  # Self-loop\n",
        "        if token.head.i != token.i:\n",
        "            A[token.i, token.head.i] = 1\n",
        "            A[token.head.i, token.i] = 1\n",
        "\n",
        "    # Node feature matrix\n",
        "    Q = contextualized_representations[:num_words, :]\n",
        "\n",
        "    return A, Q"
      ],
      "metadata": {
        "id": "d2QP18iqTSG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_dim = 768  # Dimensi output model BERT (dimensi embedding CLS)\n",
        "hidden_dim = 128\n",
        "output_dim = 128  # Dimensi dari vektor kontekstual\n",
        "\n",
        "# Initialize BiLSTM model\n",
        "bilstm_model = BiLSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "sentence_dataset = SentenceDataset(train_data['cleaned_sentence'].tolist())  # Convert Series to list\n",
        "\n",
        "# DataLoader to handle batches of sentences\n",
        "batch_size = 1\n",
        "dataloader = DataLoader(sentence_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# List to hold contextualized representations, sentence graphs, and token lists\n",
        "contextualized_representations_list = []\n",
        "adjacency_matrices_list = []\n",
        "node_feature_matrices_list = []\n",
        "tokenized_sentences_list = []  # To store tokenized words\n",
        "\n",
        "# Set the model to evaluation mode (to deactivate dropout, etc.)\n",
        "bilstm_model.eval()\n",
        "\n",
        "# Iterate over batches of sentences with tqdm for progress\n",
        "with torch.no_grad():  # Non-training mode\n",
        "    for batch_sentences in tqdm(dataloader, desc=\"Processing Sentences\", unit=\"batch\"):\n",
        "        for sentence in batch_sentences:\n",
        "            # Create contextualized word representations and tokens for the batch\n",
        "            tokens, contextualized_representations = create_contextualized_word_representations(sentence, bert_model, tokenizer, bilstm_model)\n",
        "\n",
        "            # Build the sentence graph (adjacency matrix A and node feature matrix Q)\n",
        "            A, Q = build_sentence_graph(sentence, contextualized_representations, nlp)\n",
        "\n",
        "            # Append the tokenized words and contextualized representations to the list\n",
        "            tokenized_sentences_list.append(tokens)  # Save tokens\n",
        "            adjacency_matrices_list.append(A.numpy())  # Save adjacency matrix\n",
        "            node_feature_matrices_list.append(Q.numpy())  # Save node features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXtntL3Q8omE",
        "outputId": "071f3620-8225-488e-df59-bc52564a8301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Sentences: 100%|██████████| 188/188 [01:36<00:00,  1.95batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the adjacency matrices and node feature matrices to train_data\n",
        "train_data['adjacency_matrices'] = adjacency_matrices_list\n",
        "train_data['node_feature_matrices'] = node_feature_matrices_list\n",
        "train_data['tokens'] = tokenized_sentences_list\n",
        "\n",
        "# Check the result\n",
        "print(train_data[['cleaned_sentence', 'adjacency_matrices', 'node_feature_matrices']])\n",
        "# Print the first adjacency matrix from the train_data DataFrame\n",
        "print(train_data['node_feature_matrices'].iloc[0].shape)\n",
        "print(train_data['tokens'].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fErqnVqiCFup",
        "outputId": "cedca861-75ac-43e4-d6b6-ee0c704dc446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      cleaned_sentence  \\\n",
            "0    even awful 1 much film everything form start f...   \n",
            "2    I ve see movie time one exceptional you ll wat...   \n",
            "3    movie truly capture feel freedomand freedom in...   \n",
            "4    there s much anyone say flickthe plot quite si...   \n",
            "6    check witty honest portrayal bizarre family ha...   \n",
            "..                                                 ...   \n",
            "202  gurinda chadas semiautobiographical film 2002 ...   \n",
            "204  componenta also offer fiveyear subordinate loa...   \n",
            "205  movie bad good unintentionally funny way could...   \n",
            "206  read well story solid volckman feel fail waybr...   \n",
            "207  act film old school corny stiff irene dunne lu...   \n",
            "\n",
            "                                    adjacency_matrices  \\\n",
            "0    [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "2    [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "3    [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "4    [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "6    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...   \n",
            "..                                                 ...   \n",
            "202  [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "204  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0....   \n",
            "205  [[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0,...   \n",
            "206  [[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
            "207  [[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0,...   \n",
            "\n",
            "                                 node_feature_matrices  \n",
            "0    [[-0.18720071, 0.19020185, -0.12209993, -0.118...  \n",
            "2    [[-0.175661, 0.10039671, -0.18984556, -0.03578...  \n",
            "3    [[-0.10282996, 0.04073849, -0.15306927, -0.105...  \n",
            "4    [[-0.23013644, 0.1407328, -0.25048584, -0.0526...  \n",
            "6    [[-0.09181532, 0.0691818, -0.1903865, -0.18673...  \n",
            "..                                                 ...  \n",
            "202  [[-0.14564908, 0.03757988, -0.0957897, -0.0541...  \n",
            "204  [[-0.023174949, 0.02723493, -0.09470598, 0.056...  \n",
            "205  [[-0.19356929, 0.07670724, -0.17637348, -0.128...  \n",
            "206  [[-0.21354513, 0.085894175, -0.17034125, -0.12...  \n",
            "207  [[-0.056617558, -0.031884037, -0.06266316, -0....  \n",
            "\n",
            "[188 rows x 3 columns]\n",
            "(75, 128)\n",
            "['[CLS]', 'even', 'awful', '1', 'much', 'film', 'everything', 'form', 'start', 'finish', 'make', 'cr', '##inge', 'do', 'not', 'think', 'would', 'possible', 'cr', '##am', 'overly', 'cl', '##iche', '##d', 'moment', 'one', 'piece', 'mind', 'numb', '##ingly', 'numb', '##ingly', 'waste', 'film', '##br', 'br', 'prisoner', 'cell', 'block', 'h', 'meet', 'thunder', '##bird', 'hell', 'even', 'virgil', 'expression', 'life', 'like', 'son', '##br', 'br', 'have', 'not', 'even', 'finish', 'watch', 'i', 'm', 'oh', 'che', '##es', '##y', 'clap', '3', 'actor', 'backdrop', 'do', 'child', 'adobe', 'premiere', 'truly', 'end', 'i', 've', 'start', 'ill', 'finish', 'watch', 'phase', '##br', 'br', 'oh', 'joy', 'credit', 'come', 'rescue', 'relax', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # *2 karena bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.linear(lstm_out)\n",
        "        return out\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "def get_sentence_vectors(sentence, bert_model, tokenizer):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**inputs)\n",
        "\n",
        "    # Ambil semua token embeddings dan juga ID token untuk mendeteksi kata-kata\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
        "\n",
        "    # Hapus [CLS] dan [SEP] dari tokens dan embeddings\n",
        "    if \"[CLS]\" in tokens: tokens.remove(\"[CLS]\")\n",
        "    if \"[SEP]\" in tokens: tokens.remove(\"[SEP]\")\n",
        "\n",
        "    embeddings = bert_outputs.last_hidden_state.squeeze(0)[1:-1]  # Hapus embedding untuk [CLS] dan [SEP]\n",
        "\n",
        "    return tokens, embeddings\n",
        "\n",
        "def create_contextualized_word_representations(sentence, bert_model, tokenizer, bilstm_model):\n",
        "    tokens, sentence_vector = get_sentence_vectors(sentence, bert_model, tokenizer)\n",
        "    contextualized_word_representations = bilstm_model(sentence_vector.unsqueeze(0))  # Menambah dimensi batch\n",
        "    return tokens, contextualized_word_representations.squeeze(0)  # Menghapus dimensi batch\n",
        "\n",
        "# Function to build sentence graph\n",
        "def build_sentence_graph(sentence, tokens, contextualized_representations, nlp):\n",
        "    doc = nlp(sentence)\n",
        "    num_words = len(tokens)\n",
        "\n",
        "    # Initialize adjacency matrix\n",
        "    A = torch.zeros((num_words, num_words))\n",
        "\n",
        "    # Fill adjacency matrix based on dependency parse\n",
        "    for token in doc:\n",
        "        if token.text in tokens:  # Sesuaikan dengan token dari BERT\n",
        "            idx = tokens.index(token.text)  # Cari indeks token\n",
        "            A[idx, idx] = 1  # Self-loop\n",
        "            if token.head.text in tokens:\n",
        "                head_idx = tokens.index(token.head.text)\n",
        "                A[idx, head_idx] = 1\n",
        "                A[head_idx, idx] = 1\n",
        "\n",
        "    # Node feature matrix\n",
        "    Q = contextualized_representations[:num_words, :]  # Sesuaikan ukuran dengan token yang sesuai\n",
        "\n",
        "    return A, Q\n",
        "\n",
        "# Parameters\n",
        "input_dim = 768  # Dimensi output model BERT (dimensi embedding CLS)\n",
        "hidden_dim = 128\n",
        "output_dim = 128  # Dimensi dari vektor kontekstual\n",
        "\n",
        "# Initialize BiLSTM model\n",
        "bilstm_model = BiLSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "sentence_dataset = SentenceDataset(train_data['cleaned_sentence'].tolist())  # Convert Series to list\n",
        "\n",
        "# DataLoader to handle batches of sentences\n",
        "batch_size = 1\n",
        "dataloader = DataLoader(sentence_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# List to hold contextualized representations and sentence graphs\n",
        "tokenized_sentences_list = []\n",
        "contextualized_representations_list = []\n",
        "adjacency_matrices_list = []\n",
        "node_feature_matrices_list = []\n",
        "\n",
        "# Set the model to evaluation mode (to deactivate dropout, etc.)\n",
        "bilstm_model.eval()\n",
        "\n",
        "# Iterate over batches of sentences with tqdm for progress\n",
        "with torch.no_grad():  # Non-training mode\n",
        "    for batch_sentences in tqdm(dataloader, desc=\"Processing Sentences\", unit=\"batch\"):\n",
        "        for sentence in batch_sentences:\n",
        "            # Create contextualized word representations for the batch\n",
        "            tokens, contextualized_representations = create_contextualized_word_representations(sentence, bert_model, tokenizer, bilstm_model)\n",
        "\n",
        "            # Build the sentence graph (adjacency matrix A and node feature matrix Q)\n",
        "            A, Q = build_sentence_graph(sentence, tokens, contextualized_representations, nlp)\n",
        "\n",
        "            # Append the tokenized words and contextualized representations to the list\n",
        "            tokenized_sentences_list.append(tokens)  # Save tokens\n",
        "            adjacency_matrices_list.append(A.numpy())  # Save adjacency matrix\n",
        "            node_feature_matrices_list.append(Q.numpy())  # Save node features\n",
        "\n",
        "# Add the tokenized sentences, adjacency matrices, and node feature matrices to train_data\n",
        "train_data['tokens'] = tokenized_sentences_list\n",
        "train_data['adjacency_matrices'] = adjacency_matrices_list\n",
        "train_data['node_feature_matrices'] = node_feature_matrices_list\n",
        "\n",
        "# Check the result\n",
        "print(train_data[['cleaned_sentence', 'tokens', 'adjacency_matrices', 'node_feature_matrices']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6iVBKs5XMsB",
        "outputId": "ccc8cd03-b884-4977-dee5-025406308ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Sentences: 100%|██████████| 188/188 [01:38<00:00,  1.92batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      cleaned_sentence  \\\n",
            "0    even awful 1 much film everything form start f...   \n",
            "2    I ve see movie time one exceptional you ll wat...   \n",
            "3    movie truly capture feel freedomand freedom in...   \n",
            "4    there s much anyone say flickthe plot quite si...   \n",
            "6    check witty honest portrayal bizarre family ha...   \n",
            "..                                                 ...   \n",
            "202  gurinda chadas semiautobiographical film 2002 ...   \n",
            "204  componenta also offer fiveyear subordinate loa...   \n",
            "205  movie bad good unintentionally funny way could...   \n",
            "206  read well story solid volckman feel fail waybr...   \n",
            "207  act film old school corny stiff irene dunne lu...   \n",
            "\n",
            "                                                tokens  \\\n",
            "0    [even, awful, 1, much, film, everything, form,...   \n",
            "2    [i, ve, see, movie, time, one, exceptional, yo...   \n",
            "3    [movie, truly, capture, feel, freedom, ##and, ...   \n",
            "4    [there, s, much, anyone, say, flick, ##the, pl...   \n",
            "6    [check, witty, honest, portrayal, bizarre, fam...   \n",
            "..                                                 ...   \n",
            "202  [gu, ##rin, ##da, chad, ##as, semi, ##au, ##to...   \n",
            "204  [component, ##a, also, offer, five, ##year, su...   \n",
            "205  [movie, bad, good, un, ##int, ##ent, ##ional, ...   \n",
            "206  [read, well, story, solid, vol, ##ckman, feel,...   \n",
            "207  [act, film, old, school, corn, ##y, stiff, ire...   \n",
            "\n",
            "                                    adjacency_matrices  \\\n",
            "0    [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...   \n",
            "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "3    [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "4    [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "6    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...   \n",
            "..                                                 ...   \n",
            "202  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "204  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "205  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "206  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...   \n",
            "207  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0,...   \n",
            "\n",
            "                                 node_feature_matrices  \n",
            "0    [[-0.047341235, 0.13340798, 0.0833516, 0.16387...  \n",
            "2    [[0.109229885, 0.13204534, 0.06981023, 0.21024...  \n",
            "3    [[-0.06624998, 0.24623434, 0.14093946, 0.14223...  \n",
            "4    [[-0.06772429, 0.19688992, 0.049133487, 0.2089...  \n",
            "6    [[-0.0985968, 0.24440512, 0.0095458925, 0.0206...  \n",
            "..                                                 ...  \n",
            "202  [[-0.110519975, 0.03617993, 0.019915089, 0.021...  \n",
            "204  [[-0.05000531, 0.013684162, 0.045982108, 0.071...  \n",
            "205  [[-0.074046895, 0.19451065, 0.06671614, 0.1855...  \n",
            "206  [[-0.14512771, 0.25232163, 0.01966054, 0.17676...  \n",
            "207  [[-0.2036812, 0.12520784, 0.01080928, 0.164486...  \n",
            "\n",
            "[188 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN Model\n",
        "\n"
      ],
      "metadata": {
        "id": "saFIdLJulbck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, H, A):\n",
        "        # Multiplying Adjacency matrix with Node Features first\n",
        "        support = torch.matmul(A, H)\n",
        "        # Now applying the linear transformation with weight matrix W\n",
        "        out = self.linear(support)\n",
        "        return F.relu(out)\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
        "        self.gcn2 = GCNLayer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, H, A_hat):\n",
        "        H1 = self.gcn1(H, A_hat)\n",
        "        H2 = self.gcn2(H1, A_hat)\n",
        "        return H2\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, gcn_input_dim, gcn_hidden_dim, gcn_output_dim, num_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.gcn = GCN(gcn_input_dim, gcn_hidden_dim, gcn_output_dim)\n",
        "        self.fc = nn.Linear(gcn_output_dim, num_classes)\n",
        "\n",
        "    def forward(self, H, A_hat):\n",
        "        gcn_output = self.gcn(H, A_hat)\n",
        "        # Apply pooling across node features (average pooling over nodes)\n",
        "        gcn_output_pooled = torch.mean(gcn_output, dim=0)\n",
        "        logits = self.fc(gcn_output_pooled)\n",
        "        return F.log_softmax(logits, dim=1)"
      ],
      "metadata": {
        "id": "P_q5IXGHllN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}